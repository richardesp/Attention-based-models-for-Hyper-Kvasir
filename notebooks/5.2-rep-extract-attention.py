# -*- coding: utf-8 -*-
"""5.2-rep-extract-attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KMRB9dYRiy1GciyOwY7YhGqT2yvEOFaj
"""

!pip install -U keras-cv-attention-models # Library to use the pre-trained models
!pip install tensorflow-addons

import os
import numpy as np
import pandas as pd
import cv2

from google.colab import drive
drive.mount('/content/gdrive')

dataset_dir = "gdrive/MyDrive/hyper-kvasir-dataset-green-patches"

def get_dataCategories(dataset_dir):
    import glob

    categories = []
    for folder_name in os.listdir(dataset_dir):
        if os.path.isdir(os.path.join(dataset_dir, folder_name)):
            nbr_files = len(
                glob.glob(os.path.join(dataset_dir, folder_name) + "/*.jpg")
            )
            categories.append(np.array([folder_name, nbr_files]))

    categories.sort(key=lambda a: a[0])
    cat = np.array(categories)

    return list(cat[:, 0]), list(cat[:, 1])

categories, nbr_files = get_dataCategories(dataset_dir)

# Create DataFrame
df = pd.DataFrame({"categorie": categories, "numbre of files": nbr_files})
print("number of categories: ", len(categories))
df

def get_x_y(datadir, categories, img_wid, img_high):
    stop = 5
    count = 0
    
    X, y = [], []
    for category in categories:
        if count >= stop:
          break

        path = os.path.join(datadir, category)
        class_num = categories.index(category)
        for img in os.listdir(path):

            if count >= stop:
              break

            print(f"Current image: {img}")

            try:
                img_array = cv2.imread(os.path.join(path, img))
                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
                ima_resize_rgb = cv2.resize(img_array, (img_wid, img_high))

                X.append(ima_resize_rgb)
                y.append(class_num)

            except Exception as e:
                print(e)

            count += 1


    y = np.array(y)
    X = np.array(X)

    # reshape X into img_wid x img_high x 3
    X = X.reshape(X.shape[0], img_wid, img_high, 3)

    return X, y


img_wid, img_high = 224, 224
X, y = get_x_y(dataset_dir, categories, img_wid, img_high)

print(f"X: {X.shape}")
print(f"y: {y.shape}")

import torch

X = torch.from_numpy(X).permute(0, 3, 1, 2).float()  # Cambia el orden de las dimensiones a (N, C, H, W)
y = torch.from_numpy(y).long()  # Convierte las etiquetas a tensores long

from torch.utils.data import Dataset

class CustomImageDataset(Dataset):
    def __init__(self, images, labels):
        self.images = images
        self.labels = labels

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        return self.images[idx], self.labels[idx]

dataset = CustomImageDataset(X, y)

from torch.utils.data import DataLoader

batch_size = 4
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

"""# Extracting attention"""

import matplotlib.pyplot as plt

model_path = 'gdrive/MyDrive/Universidad/TFG/models/Oversampling/Green Patches/MobileViT V2 Large/MobileViTV2Large-split_0-fiery-sweep-6-f1_macro.h5'

image_path = "gdrive/MyDrive/hyper-kvasir-dataset-green-patches/bbps-0-1/0a73edf2-98a8-4ab6-8e07-db17b241da1c.jpg"

img_array = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)

# La imagen inicialmente se carga en formato BGR, y necesito
# formato RGB

img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)

plt.matshow(img_array)

new_array = cv2.resize(img_array, (224, 224))

# 180x180 con 3 canales
new_array = new_array.reshape(-1, 224, 224, 3)

from keras_cv_attention_models import mobilevit

input_shape = (224, 224, 3)
num_classes = 23

model = mobilevit.MobileViT_V2_200(input_shape=(input_shape), num_classes=num_classes)
model.load_weights(model_path)

for layer in model.layers:
  print(layer.name)

from keras.models import Model

# Asumiendo que 'model' es tu modelo EfficientNet original
# y 'attention_layer' es el nombre de la capa de atención que quieres acceder
attention_layer = 'stack4_block3_attn_mhsa_qkv_conv'
attention_output = model.get_layer(attention_layer).output

# Crear un nuevo modelo que devuelva tanto las predicciones como los puntajes de atención
attention_model = Model(inputs=model.input, outputs=[model.output, attention_output])

import numpy as np
import matplotlib.pyplot as plt

# Selecciona la imagen de interés
image_index = 10  # Cambia esto al índice de la imagen que deseas visualizar
image = X[image_index]

# Predecir la atención para la imagen seleccionada
# Asegúrate de que la imagen tenga la forma correcta esperada por el modelo (1, height, width, channels)
predictions, attention_scores = attention_model.predict(image[np.newaxis, ...])

# Los siguientes pasos son los mismos que en el ejemplo anterior

# Calcular el mapa de atención promediando los puntajes de atención a lo largo del eje de la cabeza
attention_map = np.mean(attention_scores, axis=-1)

# Redimensionar el mapa de atención para que tenga las mismas dimensiones que la imagen original
attention_map_resized = np.resize(attention_map, (224, 224))

# Normalizar el mapa de atención para que los valores estén en el rango [0, 1]
attention_map_normalized = (attention_map_resized - np.min(attention_map_resized)) / (np.max(attention_map_resized) - np.min(attention_map_resized))

# Superponer el mapa de atención en la imagen original
overlay = np.repeat(attention_map_normalized[..., np.newaxis], 3, axis=-1)
overlayed_image = np.clip(image + 0.5 * overlay, 0, 1)

# Mostrar la imagen original y la imagen con el mapa de atención superpuesto
fig, ax = plt.subplots(1, 2, figsize=(10, 5))
ax[0].imshow(image)
ax[0].set_title("Imagen original")
ax[1].imshow(overlayed_image)
ax[1].set_title("Imagen con atención superpuesta")
plt.show()

for layer in model.layers:

  from keras.models import Model

  # Asumiendo que 'model' es tu modelo EfficientNet original
  # y 'attention_layer' es el nombre de la capa de atención que quieres acceder
  attention_layer = layer
  attention_output = model.get_layer(attention_layer).output

  # Crear un nuevo modelo que devuelva tanto las predicciones como los puntajes de atención
  attention_model = Model(inputs=model.input, outputs=[model.output, attention_output])

  print(f"Layer: {layer.name}")

  import numpy as np
  import matplotlib.pyplot as plt

  # Selecciona la imagen de interés
  image_index = 10  # Cambia esto al índice de la imagen que deseas visualizar
  image = X[image_index]

  # Predecir la atención para la imagen seleccionada
  # Asegúrate de que la imagen tenga la forma correcta esperada por el modelo (1, height, width, channels)
  predictions, attention_scores = attention_model.predict(image[np.newaxis, ...])

  # Los siguientes pasos son los mismos que en el ejemplo anterior

  # Calcular el mapa de atención promediando los puntajes de atención a lo largo del eje de la cabeza
  attention_map = np.mean(attention_scores, axis=-1)

  # Redimensionar el mapa de atención para que tenga las mismas dimensiones que la imagen original
  attention_map_resized = np.resize(attention_map, (224, 224))

  # Normalizar el mapa de atención para que los valores estén en el rango [0, 1]
  attention_map_normalized = (attention_map_resized - np.min(attention_map_resized)) / (np.max(attention_map_resized) - np.min(attention_map_resized))

  # Superponer el mapa de atención en la imagen original
  overlay = np.repeat(attention_map_normalized[..., np.newaxis], 3, axis=-1)
  overlayed_image = np.clip(image + 0.5 * overlay, 0, 1)

  # Mostrar la imagen original y la imagen con el mapa de atención superpuesto
  fig, ax = plt.subplots(1, 2, figsize=(10, 5))
  ax[0].imshow(image)
  ax[0].set_title("Imagen original")
  ax[1].imshow(overlayed_image)
  ax[1].set_title("Imagen con atención superpuesta")
  plt.show()